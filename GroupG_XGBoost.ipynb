{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader.data as pdd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "from ta import *\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, auc, average_precision_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=dt.datetime(2010,1,1)\n",
    "end=dt.datetime(2019,11,27)\n",
    "lst=['AAPL','AMZN','MSFT','JPM','GOOGL','BA','LMT','WMT','C','IBM','MCO','UAL','BBY','BLK','NVDA','MCK','MRK','XOM','ORCL','NKE']\n",
    "dfdc={}\n",
    "for i in lst:\n",
    "    df = pdd.DataReader(i, \"quandl\",start,end,api_key='xxxxxxxxx')\n",
    "    df = df.reindex(index=df.index[::-1])\n",
    "    df.drop(df.loc[:, 'Open':'SplitRatio'].columns, inplace=True, axis = 1)\n",
    "    dfdc[i] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creat indicators and label the target as \"1\" and \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## creat a copy file to train the model\n",
    "stockDict = copy.deepcopy(dfdc) # copy of dfdc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## creat indicators\n",
    "for stock in stockDict:\n",
    "    # Relative Strength Index (RSI)\n",
    "    stockDict[stock]['RSI'] = momentum.rsi(stockDict[stock]['AdjClose'])\n",
    "    # Average Directional Movement Index (ADX)\n",
    "    stockDict[stock]['ADX'] = trend.adx(stockDict[stock]['AdjHigh'], stockDict[stock]['AdjLow'], stockDict[stock]['AdjClose'])\n",
    "    # Parabolic Stop and Reverse (Parabolic SAR)\n",
    "    stockDict[stock]['PSAR_UP'] = trend.psar_up(stockDict[stock]['AdjHigh'], stockDict[stock]['AdjLow'], stockDict[stock]['AdjClose'])\n",
    "    stockDict[stock]['PSAR_DOWN'] = trend.psar_down(stockDict[stock]['AdjHigh'], stockDict[stock]['AdjLow'], stockDict[stock]['AdjClose'])\n",
    "    # Exponential Moving Average (5days)\n",
    "    stockDict[stock]['EMA5'] = trend.ema_indicator(stockDict[stock]['AdjClose'],n=5)\n",
    "    # difference between EMA (5 days) and close price\n",
    "    stockDict[stock]['EMA5_AdjClose'] = stockDict[stock]['EMA5'] - stockDict[stock]['AdjClose']\n",
    "    #average true range\n",
    "    stockDict[stock]['ATR'] = volatility.average_true_range(stockDict[stock]['AdjHigh'], stockDict[stock]['AdjLow'], stockDict[stock]['AdjClose'], n=14)\n",
    "    #Moving Average Convergence Divergence\n",
    "    stockDict[stock]['MACD'] = trend.macd(stockDict[stock]['AdjClose'], n_fast=12,n_slow=26)\n",
    "    #Stochastic Oscillator\n",
    "    stockDict[stock]['SR'] = momentum.stoch(stockDict[stock]['AdjHigh'],stockDict[stock]['AdjLow'], stockDict[stock]['AdjClose'], n=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### lag indicators (5days)\n",
    "lags = 5\n",
    "keys = stockDict['AAPL'].keys()[5:14]\n",
    "for stock in stockDict:\n",
    "    keys_new = []\n",
    "    for key in keys:\n",
    "        for lag in range(1,lags+1):\n",
    "            key_new = key + '_' + str(lag)\n",
    "            stockDict[stock][key_new] = stockDict[stock][key].shift(lag)\n",
    "            keys_new.append(key_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ###### uncomment this line to see the features used to predict the target\n",
    "# keys_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### target label: up_down \n",
    "for stock in stockDict:\n",
    "    stockDict[stock]['Return'] = np.log(stockDict[stock]['AdjClose']/stockDict[stock]['AdjClose'].shift(1))\n",
    "    stockDict[stock]['Up_Down'] = np.where(stockDict[stock]['Return'] > 0, 1, 0)  # 1 if up, 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_test split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, split_ratio):\n",
    "    \n",
    "    trainSize = int(split_ratio*df.shape[0])\n",
    "    train_set = df[:trainSize]\n",
    "    test_set = df[trainSize:]\n",
    "    \n",
    "    X_train = train_set[keys_new]\n",
    "    X_train = scaler.fit_transform(X_train) #normalize\n",
    "    \n",
    "    X_test = test_set[keys_new]\n",
    "    X_test = scaler.transform(X_test)  #normalize\n",
    "    \n",
    "    y_train = train_set['Up_Down']\n",
    "    y_test = test_set['Up_Down']\n",
    "    \n",
    "#     print(X_train.shape,y_train.shape)\n",
    "#     print(X_test.shape, y_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearch Function to tune the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate' :[0.15,0.20,0.25,0.30, 0.35],\n",
    "    'n_estimators':[100,200,500,1000],\n",
    "    'max_depth':[3,4,5,6,7,8],\n",
    "    'min_child_weight':[1,3,5,7],\n",
    "    'gamma':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    'colsample_bytree':[0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random search of hyperparameters\n",
    "def XGB_best (X_train,X_test,y_train,y_test, params):\n",
    "    \n",
    "    Classifier = xgb.XGBClassifier()\n",
    "    random_search = RandomizedSearchCV(Classifier,param_distributions = params, \\\n",
    "                                       scoring = 'roc_auc', cv=5, random_state = 666)\n",
    "    best_xgb = random_search.fit(X_train,y_train)\n",
    "    y_pred = best_xgb.predict(X_test)\n",
    "    best_estimator = random_search.best_estimator_ \n",
    "    \n",
    "    return best_estimator, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model on train dataset and make prediction on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.47101449275362317\n",
      "confusion matrix:\n",
      "[[107  88]\n",
      " [131  88]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.55      0.49       195\n",
      "           1       0.50      0.40      0.45       219\n",
      "\n",
      "   micro avg       0.47      0.47      0.47       414\n",
      "   macro avg       0.47      0.48      0.47       414\n",
      "weighted avg       0.48      0.47      0.47       414\n",
      "\n",
      "=======================================\n",
      "AMZN\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.5120772946859904\n",
      "confusion matrix:\n",
      "[[ 86  93]\n",
      " [109 126]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.48      0.46       179\n",
      "           1       0.58      0.54      0.56       235\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       414\n",
      "   macro avg       0.51      0.51      0.51       414\n",
      "weighted avg       0.52      0.51      0.51       414\n",
      "\n",
      "=======================================\n",
      "MSFT\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.5277108433734939\n",
      "confusion matrix:\n",
      "[[129  61]\n",
      " [135  90]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.68      0.57       190\n",
      "           1       0.60      0.40      0.48       225\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       415\n",
      "   macro avg       0.54      0.54      0.52       415\n",
      "weighted avg       0.55      0.53      0.52       415\n",
      "\n",
      "=======================================\n",
      "JPM\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.44096385542168676\n",
      "confusion matrix:\n",
      "[[140  51]\n",
      " [181  43]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.73      0.55       191\n",
      "           1       0.46      0.19      0.27       224\n",
      "\n",
      "   micro avg       0.44      0.44      0.44       415\n",
      "   macro avg       0.45      0.46      0.41       415\n",
      "weighted avg       0.45      0.44      0.40       415\n",
      "\n",
      "=======================================\n",
      "GOOGL\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.4795180722891566\n",
      "confusion matrix:\n",
      "[[134  58]\n",
      " [158  65]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.70      0.55       192\n",
      "           1       0.53      0.29      0.38       223\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       415\n",
      "   macro avg       0.49      0.49      0.46       415\n",
      "weighted avg       0.50      0.48      0.46       415\n",
      "\n",
      "=======================================\n",
      "BA\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.4650602409638554\n",
      "confusion matrix:\n",
      "[[127  43]\n",
      " [179  66]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.75      0.53       170\n",
      "           1       0.61      0.27      0.37       245\n",
      "\n",
      "   micro avg       0.47      0.47      0.47       415\n",
      "   macro avg       0.51      0.51      0.45       415\n",
      "weighted avg       0.53      0.47      0.44       415\n",
      "\n",
      "=======================================\n",
      "LMT\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.5084337349397591\n",
      "confusion matrix:\n",
      "[[ 38 144]\n",
      " [ 60 173]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.21      0.27       182\n",
      "           1       0.55      0.74      0.63       233\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       415\n",
      "   macro avg       0.47      0.48      0.45       415\n",
      "weighted avg       0.48      0.51      0.47       415\n",
      "\n",
      "=======================================\n",
      "WMT\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.491566265060241\n",
      "confusion matrix:\n",
      "[[ 92  91]\n",
      " [120 112]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.50      0.47       183\n",
      "           1       0.55      0.48      0.51       232\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       415\n",
      "   macro avg       0.49      0.49      0.49       415\n",
      "weighted avg       0.50      0.49      0.49       415\n",
      "\n",
      "=======================================\n",
      "C\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.5132530120481927\n",
      "confusion matrix:\n",
      "[[133  64]\n",
      " [138  80]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.68      0.57       197\n",
      "           1       0.56      0.37      0.44       218\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       415\n",
      "   macro avg       0.52      0.52      0.51       415\n",
      "weighted avg       0.52      0.51      0.50       415\n",
      "\n",
      "=======================================\n",
      "IBM\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.5084337349397591\n",
      "confusion matrix:\n",
      "[[ 87 115]\n",
      " [ 89 124]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.43      0.46       202\n",
      "           1       0.52      0.58      0.55       213\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       415\n",
      "   macro avg       0.51      0.51      0.50       415\n",
      "weighted avg       0.51      0.51      0.51       415\n",
      "\n",
      "=======================================\n",
      "MCO\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.4578313253012048\n",
      "confusion matrix:\n",
      "[[106  77]\n",
      " [148  84]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.58      0.49       183\n",
      "           1       0.52      0.36      0.43       232\n",
      "\n",
      "   micro avg       0.46      0.46      0.46       415\n",
      "   macro avg       0.47      0.47      0.46       415\n",
      "weighted avg       0.48      0.46      0.45       415\n",
      "\n",
      "=======================================\n",
      "UAL\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.5132530120481927\n",
      "confusion matrix:\n",
      "[[143  58]\n",
      " [144  70]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.71      0.59       201\n",
      "           1       0.55      0.33      0.41       214\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       415\n",
      "   macro avg       0.52      0.52      0.50       415\n",
      "weighted avg       0.52      0.51      0.49       415\n",
      "\n",
      "=======================================\n",
      "BBY\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.4843373493975904\n",
      "confusion matrix:\n",
      "[[109  72]\n",
      " [142  92]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.60      0.50       181\n",
      "           1       0.56      0.39      0.46       234\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       415\n",
      "   macro avg       0.50      0.50      0.48       415\n",
      "weighted avg       0.51      0.48      0.48       415\n",
      "\n",
      "=======================================\n",
      "BLK\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.4819277108433735\n",
      "confusion matrix:\n",
      "[[159  39]\n",
      " [176  41]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.80      0.60       198\n",
      "           1       0.51      0.19      0.28       217\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       415\n",
      "   macro avg       0.49      0.50      0.44       415\n",
      "weighted avg       0.49      0.48      0.43       415\n",
      "\n",
      "=======================================\n",
      "NVDA\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.5493975903614458\n",
      "confusion matrix:\n",
      "[[ 24 161]\n",
      " [ 26 204]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.13      0.20       185\n",
      "           1       0.56      0.89      0.69       230\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       415\n",
      "   macro avg       0.52      0.51      0.44       415\n",
      "weighted avg       0.52      0.55      0.47       415\n",
      "\n",
      "=======================================\n",
      "MCK\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.4867469879518072\n",
      "confusion matrix:\n",
      "[[ 77 129]\n",
      " [ 84 125]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.37      0.42       206\n",
      "           1       0.49      0.60      0.54       209\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       415\n",
      "   macro avg       0.49      0.49      0.48       415\n",
      "weighted avg       0.49      0.49      0.48       415\n",
      "\n",
      "=======================================\n",
      "MRK\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.5156626506024097\n",
      "confusion matrix:\n",
      "[[134  74]\n",
      " [127  80]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.64      0.57       208\n",
      "           1       0.52      0.39      0.44       207\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       415\n",
      "   macro avg       0.52      0.52      0.51       415\n",
      "weighted avg       0.52      0.52      0.51       415\n",
      "\n",
      "=======================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOM\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.5084337349397591\n",
      "confusion matrix:\n",
      "[[117  84]\n",
      " [120  94]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.58      0.53       201\n",
      "           1       0.53      0.44      0.48       214\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       415\n",
      "   macro avg       0.51      0.51      0.51       415\n",
      "weighted avg       0.51      0.51      0.51       415\n",
      "\n",
      "=======================================\n",
      "ORCL\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.46987951807228917\n",
      "confusion matrix:\n",
      "[[128  66]\n",
      " [154  67]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.66      0.54       194\n",
      "           1       0.50      0.30      0.38       221\n",
      "\n",
      "   micro avg       0.47      0.47      0.47       415\n",
      "   macro avg       0.48      0.48      0.46       415\n",
      "weighted avg       0.48      0.47      0.45       415\n",
      "\n",
      "=======================================\n",
      "NKE\n",
      "=======================================\n",
      "accuracy score:\n",
      "0.4939759036144578\n",
      "confusion matrix:\n",
      "[[ 97 107]\n",
      " [103 108]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.48      0.48       204\n",
      "           1       0.50      0.51      0.51       211\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       415\n",
      "   macro avg       0.49      0.49      0.49       415\n",
      "weighted avg       0.49      0.49      0.49       415\n",
      "\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "for stock in stockDict:\n",
    "    # test_train split\n",
    "    temp = split_train_test(stockDict[stock],0.8) #split_ratio = 0.8\n",
    "    X_train = temp[0]\n",
    "    X_test = temp[1]\n",
    "    y_train = temp[2]\n",
    "    y_test = temp[3]   \n",
    "    # fit the model\n",
    "    rand_xgb = XGB_best(X_train, X_test, y_train, y_test, params)\n",
    "    y_pred = rand_xgb[1] \n",
    "    \n",
    "    print(stock)\n",
    "    print('=======================================')\n",
    "#     print(\"best parameters:\")\n",
    "#     print(rand_xgb[0]) #[0] will return the best parameters\n",
    "    print(\"accuracy score:\")\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    print(\"confusion matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"classification report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('=======================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codes below are for testing the whole training process before run the model fitting on all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Take 'Apple' as a sample \n",
    "\n",
    "# temp_ = split_train_test(stockDict['AAPL'],0.8)\n",
    "\n",
    "# aapl_train = temp_[0] #x_train\n",
    "# aapl_test = temp_[1] #x_test\n",
    "# y_tr =temp_[2] #y_train\n",
    "# y_ts = temp_[3] #y_test\n",
    "\n",
    "# # timing the gridsearch on one stock\n",
    "# import timeit \n",
    "# start = timeit.default_timer()\n",
    "# best_aapl = XGB_best(aapl_train,aapl_test,y_tr,y_ts,params)\n",
    "# stop = timeit.default_timer()\n",
    "# print('Time: ', stop - start)\n",
    "\n",
    "# y_pr = best_aapl[1]\n",
    "# accuracy_score(y_pr,y_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tried GridSearch to tune the hypermeters, which was time consuming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GridSearch\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# def XGB_Grid (X_train,X_test,y_train,y_test, params):\n",
    "    \n",
    "#     Classifier = xgb.XGBClassifier()\n",
    "#     grid_search = GridSearchCV(Classifier,param_grid = params, scoring = 'roc_auc', cv=5)\n",
    "#     best_xgb_grid = grid_search.fit(X_train,y_train)\n",
    "#     y_pred_grid = best_xgb_grid.predict(X_test)\n",
    "#     best_estimator_grid = grid_search.best_estimator_ \n",
    "    \n",
    "#     return best_estimator_grid, y_pred_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for stock in stockDict:\n",
    "#     # test_train split\n",
    "#     temp = split_train_test(stockDict[stock],0.8) #split_ratio = 0.8\n",
    "#     X_train = temp[0]\n",
    "#     X_test = temp[1]\n",
    "#     y_train = temp[2]\n",
    "#     y_test = temp[3]   \n",
    "#     # fit the model\n",
    "#     rand_xgb = XGB_Grid(X_train, X_test, y_train, y_test, params)\n",
    "#     y_pred = rand_xgb[1] #[0] will return the best parameters\n",
    "    \n",
    "#     print(stock)\n",
    "#     print('=======================================')\n",
    "#     print(\"best parameters:\")\n",
    "#     print(rand_xgb[0])\n",
    "#     print(\"accuracy score:\")\n",
    "#     print(accuracy_score(y_test, y_pred))\n",
    "#     print(\"confusion matrix:\")\n",
    "#     print(confusion_matrix(y_test, y_pred))\n",
    "#     print(\"classification report:\")\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "#     print('=======================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
